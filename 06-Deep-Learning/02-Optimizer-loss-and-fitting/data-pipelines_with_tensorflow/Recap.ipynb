{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9802572f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b9aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 17:52:06.106464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-03 17:52:06.376525: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import PIL\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# TensorFlow\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Sklearn\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Remove TF info messages like \"Plugin optimizer for device_type GPU is enabled\"\n",
    "# Also removes warnings, but not errors. Set to 1 for warnings.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddc739",
   "metadata": {},
   "source": [
    "# Data Pipelines with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073bfe56",
   "metadata": {},
   "source": [
    "üéØ **Objectives**\n",
    "- How to make pipelines with Deep Learning\n",
    "- How to load heavy data batch per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f0363",
   "metadata": {},
   "source": [
    "## **Part I: How to make pipelines with Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15148fed",
   "metadata": {},
   "source": [
    "### Load Data (the Petfinder dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf55e14",
   "metadata": {},
   "source": [
    "üëâ Let's load the **PetFinder** dataset. \n",
    "* Each row describes a pet\n",
    "* Each column describes an attribute of a pet\n",
    "\n",
    "\n",
    "üê∂ You will use this information to ***predict whether a pet will be adopted or not***. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c473648",
   "metadata": {},
   "source": [
    "| Column | Description| Feature Type | Data Type\n",
    "|------------|--------------------|----------------------|-----------------\n",
    "|Type | Type of animal (Dog, Cat) | Categorical | string\n",
    "|Age |  Age of the pet | Numerical | integer\n",
    "|Breed1 | Primary breed of the pet | Categorical | string\n",
    "|Color1 | Color 1 of pet | Categorical | string\n",
    "|Color2 | Color 2 of pet | Categorical | string\n",
    "|MaturitySize | Size at maturity | Categorical | string\n",
    "|FurLength | Fur length | Categorical | string\n",
    "|Vaccinated | Pet has been vaccinated | Categorical | string\n",
    "|Sterilized | Pet has been sterilized | Categorical | string\n",
    "|Health | Health Condition | Categorical | string\n",
    "|Fee | Adoption Fee | Numerical | integer\n",
    "|Description | Profile write-up for this pet | Text | string\n",
    "|PhotoAmt | Total uploaded photos for this pet | Numerical | integer\n",
    "|AdoptionSpeed | Speed of adoption | Classification | integer\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/petfinder.csv\")\n",
    "pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d47e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pets.target.value_counts(normalize = True), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "train, test = train_test_split(pets, test_size=0.2)\n",
    "\n",
    "# Train-Val Split\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and target in the Train, Val and Test Set\n",
    "\n",
    "X_train = train.drop(columns='target')\n",
    "y_train = train['target']\n",
    "\n",
    "X_val = val.drop(columns='target')\n",
    "y_val = val['target']\n",
    "\n",
    "X_test = test.drop(columns='target')\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf121c13",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Our dataset has both ***numerical*** and ***categorical values***. \n",
    "\n",
    "As for any Machine Learning / Deep Learning model, we need to preprocess them before training the model.\n",
    "\n",
    "üë®üèª‚Äçüè´ You have three options:\n",
    "\n",
    "* **(A)** Use Scikit-Learn to preprocess the dataset before feeding a Neural Network (no Pipeline)  \n",
    "* **(B)** Wrap your Neural Network into a Scikit-Learn estimator and use a Scikit-Learn Pipeline\n",
    "* **(C)** Use full Tensorflow pipelines  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbea30",
   "metadata": {},
   "source": [
    "### (A) 0Ô∏è‚É£ No pipeline\n",
    "\n",
    "1. Preprocess data with Scikit-Learn \n",
    "2. Feed your Neural Network with the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1feeb",
   "metadata": {},
   "source": [
    "#### (A.1) Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7ffac",
   "metadata": {},
   "source": [
    "‚ùì Create `X_train_preproc`, `X_val_preproc`, `X_test_preproc` ‚ùì\n",
    "\n",
    "* Scaling numerical features\n",
    "* Encoding categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17ea6a",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc38fe6",
   "metadata": {},
   "source": [
    "#### (A.2) Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ba34d",
   "metadata": {},
   "source": [
    "‚ùì **Questions** ‚ùì\n",
    "* Design (Architecture + Compile) a Neural Network\n",
    "* Fit it on the training set\n",
    "* Evaluate its performance on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd924ab",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3592a",
   "metadata": {},
   "source": [
    "### (B) üëª Wrapping a Neural Net into a Sklearn estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e0e94",
   "metadata": {},
   "source": [
    "üëâ _Pipeline_ and _ColumnTransformer_ are designed for Machine Learning Models.\n",
    "\n",
    "üëª So, how about disguising a Neural Network into a Scikit-Learn estimator to use the aforementioned tools? It is possible! We can treat a Tensorflow.Keras model as a Scikit-Learn estimator using üìö [**Keras Wrappers**](https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn) üìö\n",
    "\n",
    "üî• Now, we can **_CrossValidate_, _GridSearchCV_, _RandomizedSearchCV_ a Deep Learning model which is wrapped**. üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd2428",
   "metadata": {},
   "source": [
    "#### (B.1) Introduction to Keras Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7df22",
   "metadata": {},
   "source": [
    "‚ùì Run the cells below and try understand the syntax ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    ###########################\n",
    "    #             1. Define architecture              #\n",
    "    ###########################\n",
    "\n",
    "    # Notice that we don't specify the input shape yet\n",
    "    # as we don't know the shape post-preprocessing!\n",
    "    # One consequence is that here, you cannot yet print\n",
    "    # the model's summary. It will be known after fitting it\n",
    "    # to X_train_preprocessed, y_train\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(32, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(15, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    ###########################\n",
    "    #                 2. Compile model                 #\n",
    "    ###########################\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer = 'adam',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is \"Halloween\" time\n",
    "# Disguise your Deep Learning Model into a Scikit Learn model\n",
    "\n",
    "disguised_deep_model = KerasClassifier(\n",
    "    build_fn = create_model,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908d834",
   "metadata": {},
   "source": [
    "#### (B.2) Cross-Validating a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7930d0b",
   "metadata": {},
   "source": [
    "‚ùì Evaluate/CrossValidate your estimator on your training set already preprocessed ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e180c63",
   "metadata": {},
   "source": [
    "#### (B.3) Pipelining a Wrapped Deep Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac15c9",
   "metadata": {},
   "source": [
    "‚ùì Wrap your model within a pipeline including a preprocessing step and evaluate it directly on the raw data this time (not on the preprocessed data)‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f96d4c",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839ebfb",
   "metadata": {},
   "source": [
    "#### (B.4) GridSearchCV on a Wrapped Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef53818",
   "metadata": {},
   "source": [
    "Now that our model is pipelined, we can even **GridSearchCV** its hyper-parameters üî•\n",
    "\n",
    "‚ùì Run the cells below to understand how the syntax works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_grid(activation = 'relu', optimizer = 'rmsprop'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(32, activation=activation))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(15, activation=activation))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid = KerasClassifier(\n",
    "    build_fn = create_model_grid,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    verbose = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ccaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_grid = make_pipeline(preproc, model_grid)\n",
    "#pipe_grid.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now, we can grid-search the hyper-params of everything\n",
    "# From the preprocessing, the architecture, the compiler, and the fit!\n",
    "\n",
    "param_grid = dict(\n",
    "    columntransformer__standardscaler__with_mean = [True, False], # Preprocessing hyperparams\n",
    "    kerasclassifier__activation = ['tanh', 'relu'],               # Architecture hyperparams\n",
    "    kerasclassifier__optimizer = [\"adam\", \"rmsprop\"],             # Compiler hyperparams\n",
    "    kerasclassifier__batch_size = [8, 64],                        # Fit hyperparams\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator = pipe_grid,\n",
    "    param_grid = param_grid,\n",
    "    cv = 2,\n",
    "    verbose = 2,\n",
    "    n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066902b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cd6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(grid.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a33143",
   "metadata": {},
   "source": [
    "### (C) üß® Full pipeline in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b6d6a",
   "metadata": {},
   "source": [
    "üß® This option is recommended for real projects, especially when you need:\n",
    "1. Performance or\n",
    "2. Production-Readiness\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><i>Why?</i></summary>\n",
    "\n",
    "Indeed, having all the preprocessing steps within one single TensorFlow Keras model allows you to generate one <a href=\"https://www.tensorflow.org/guide/intro_to_graphs\">**`tf.Graph`**</a> representation of your model.\n",
    "\n",
    "A **`tf.Graph`** is mandatory for:\n",
    "* distributed computations\n",
    "* and serving on many devices \n",
    "\n",
    "(using **`Tensorflow Lite`** for back-end free predictions for instance). \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e0d1c",
   "metadata": {},
   "source": [
    "The idea is to use **`Normalization layers`** and **`CategoryEncoding layers`** within your model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4220a83",
   "metadata": {},
   "source": [
    "#### (C.1) üòå If the preprocessing pipeline is sequential, everything is easy/straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74684e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "sequential_pipe = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        disguised_deep_model\n",
    ")\n",
    "\n",
    "sequential_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef989fc6",
   "metadata": {},
   "source": [
    "For instance, if there was only numerical data in our dataset üëá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we focus exclusively on numerical data and scale them:\n",
    "X_train_num = X_train.select_dtypes(exclude=['object']).values\n",
    "X_val_num = X_val.select_dtypes(exclude=['object']).values\n",
    "X_test_num = X_test.select_dtypes(exclude=['object']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02883aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. The Normalization Layer\n",
    "\n",
    "normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train_num) # \"Fit\" it on the train set\n",
    "\n",
    "# 1. The Architecture\n",
    "model = Sequential()\n",
    "model.add(normalizer) # Using the Normalization layer to standardize the data points during the forward pass\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(15, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# 2. Compiling\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# 3. Training\n",
    "model.fit(\n",
    "    X_train_num,\n",
    "    y_train,\n",
    "    validation_data = (X_val_num, y_val),\n",
    "    batch_size = 64,\n",
    "    epochs = 20,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 4. Evaluating\n",
    "model.evaluate(X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1246d",
   "metadata": {},
   "source": [
    "#### (C.2) ü§Ø if the preprocessing pipeline requires parallel column transformers, TF Sequential API `model.add(...)` is not enough... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e710b565",
   "metadata": {},
   "source": [
    "‚òùÔ∏è You will need to use TF **`Functional API`** to **`produce a Non-Sequential Neural Network`**\n",
    "\n",
    "üìö [Google's Tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) show you how to solve this exact **PetFinder** dataset with this method\n",
    "\n",
    "üìÜ We will show you an example of Non-Sequential Neural Network during the last session of Deep Learning.\n",
    "\n",
    "üßëüèª‚Äçüè´ Non-Sequential Models look something like this üëá:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9652b",
   "metadata": {},
   "source": [
    "```python\n",
    "# Numerical preprocessing model = function(X_numerical)\n",
    "model_numerical = ...  \n",
    "\n",
    "# Categorical preprocessing model = function(X_categorical)\n",
    "model_categorical = ...\n",
    "\n",
    "# Combined model\n",
    "all_features = layers.concatenate([model_numerical, model_categorical])\n",
    "\n",
    "# Then create the Dense network on the preprocessed features\n",
    "x = tf.keras.layers.Dense(8, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dense(2, activation=\"relu\")(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "model.compile(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04588dc7",
   "metadata": {},
   "source": [
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/non_sequential_models.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3921423",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Part II. How to deal with heavy datasets ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2dd6c",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Most Deep Learning projects use datasets that are **too heavy to be loaded on RAM entirely** \n",
    "\n",
    "Fortunately, we can train our network **batch per batch**!\n",
    "\n",
    "‚úÖ Tensorflow provides a powerful [**`tf.data.Dataset`**](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class that helps to deal with:\n",
    "* **data loading**\n",
    "* **processing batch-per-batch**\n",
    "\n",
    "‚úÖ Keras provides [**`tf.keras.preprocessing`**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing) wrappers around this to avoid getting your hands too dirty:\n",
    "- **`image_dataset_from_directory`**\n",
    "- **`text_dataset_from_directory`**\n",
    "- **`timeseries_dataset_from_array`**\n",
    "\n",
    "Let's illustrate this with a heavy image dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838ebe1",
   "metadata": {},
   "source": [
    "### (1) Save large files on a hard drive (local or cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46eda5e",
   "metadata": {},
   "source": [
    "‚ùì Run following cells (don't focus on the syntax here but on what is going on) ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download 229Mo of images\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "data_dir = tf.keras.utils.get_file(\n",
    "    origin = dataset_url,\n",
    "    fname = 'flower_photos',\n",
    "    untar = True\n",
    ")\n",
    "\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820af997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just unzipped and saved all the files in the following folder\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70722845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how each photo is saved in a different folder depending on its category\n",
    "! ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In total we have 229Mo of files, compressed.\n",
    "# Imagine if there was 50Go? They couldn't fit in RAM\n",
    "! du -h $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 3670 jpg images in 5 classes\n",
    "len(list(data_dir.glob('*/*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb39dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just look at one image\n",
    "sunflowers = list(data_dir.glob('sunflowers/*'))\n",
    "PIL.Image.open(str(sunflowers[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046a5a9",
   "metadata": {},
   "source": [
    "### (2) Prepare to load images in RAM memory batch per batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68256a84",
   "metadata": {},
   "source": [
    "We will use üìö <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\">**`image_dataset_from_directory`**</a> üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f111fe1",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Notice how it automatically labelled our image files into the 5 classes!\n",
    "- The folder structure is deduced from a default paramter:  **`labels='inferred'`** of the `image_dataset_from_directory`\n",
    "- You can manually pass a list of labels instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8807cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ds` is a `tf.data.Dataset` object of \"tuples\"\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fb409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset contain no real data until they are iterated over\n",
    "import sys\n",
    "sys.getsizeof(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X_batch, y_batch)  in ds:\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)\n",
    "\n",
    "    break # just show the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first image\n",
    "plt.imshow(X_batch[0]/255);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34c731",
   "metadata": {},
   "source": [
    "üìö <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/datasets\">**tf.data.Dataset**</a> üìö are just an abstraction that represent a sequence of elements. \n",
    "\n",
    "\n",
    "\n",
    "They enable us to:\n",
    "\n",
    "- Load elements batch-per-batch in memory\n",
    "- From different formats, storage places, etc...\n",
    "- Apply preprocessing on the fly (ex: shuffle, resize, and many many more)\n",
    "\n",
    "üìö [**tf/guide/data**](https://www.tensorflow.org/guide/data) üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822746a5",
   "metadata": {},
   "source": [
    "### (3) Train a model directly on a `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540505c",
   "metadata": {},
   "source": [
    "‚ùì Try to fit a very simple dense Neural Network on `ds` \n",
    "\n",
    "- You can directly call `model.fit(ds, epochs=1)`\n",
    "- Your first layer should use `layers.Flatten` to flatten a $(256,256,3)$ picture in into a ($256 \\times 256 \\times 3$) vector so acceptable for Dense layers\n",
    "- You can use **`loss='sparse_categorical_crossentropy'`**: \"*sparse_*\" avoids *\"one-hot-encoding\"* the target with `to_categorical(y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ffae2",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bfbbb",
   "metadata": {},
   "source": [
    "üí° The accuracy of this model is approx~ 20-25% which doesn't really beat the baseline 20% (1 divided by 5 categories...)\n",
    "\n",
    "ü§° Why ? **A Dense Neural Network Architecture is NOT designed for classifying images!**\n",
    "\n",
    "üöÄ In the next session, we will use **`Convolutional Neural Networks (CNN)`** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6141f0",
   "metadata": {},
   "source": [
    "### (Bonus) Proper solution to the Flower problem using CNN & Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1565b58",
   "metadata": {},
   "source": [
    "üßëüèª‚Äçüè´ Come back to this section after studying **Deep Learning > 03. Convolutional Neural Networks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(64, 64), # resize on the fly\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(64, 64), # resize on the fly\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d13faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(layers.experimental.preprocessing.Rescaling(1. / 255))\n",
    "model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[EarlyStopping(patience=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e60932",
   "metadata": {},
   "source": [
    "üèÅ Congratulations \n",
    "\n",
    "üíæ Don't forget to `git add/push/commit` your notebook.\n",
    "\n",
    "üìÜ What's next on the menu?\n",
    "\n",
    "* **Deep Learning** **`> 01. Fundamentals`** and **`> 02. Optimizers, Fit, Loss`** helped you master the foundations of Deep Learning, what are neurons, layers, architecture, loss functions, optimizers, learning rates, ... \n",
    "    * We have been working with inputs which are **row vectors** (each row has $p$ features)\n",
    "    * All the neurons from a layer are **fully connected** to the next layer\n",
    "\n",
    "* What if we want to classify pictures instead? Each picture has a certain amount of pixels that we could potentially consider as features... But it is more complex than that. See you in the next session **Deep Learning `> 03. Convolutional Neural Networks`**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
